{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36e4cfd",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "In this notebook, we will prepare the dataset for modeling.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f9ae9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a82abde",
   "metadata": {},
   "source": [
    "**A. Data daily**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ca9e5a",
   "metadata": {},
   "source": [
    "**1. Overview of Data after Data Understanding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8e48ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_daily = pd.read_excel(r'../data/processed/data_daily_after_basic_understand.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d9a8cbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: ['tempmax', 'tempmin', 'temp', 'feelslikemax', 'feelslikemin', 'feelslike', 'dew', 'humidity', 'precip', 'precipcover', 'windgust', 'windspeed', 'winddir', 'sealevelpressure', 'cloudcover', 'visibility', 'solarradiation', 'solarenergy', 'uvindex', 'severerisk', 'moonphase']\n",
      "Categorical features: ['conditions', 'description', 'icon', 'stations']\n"
     ]
    }
   ],
   "source": [
    "# Xác định loại dữ liệu\n",
    "numerical_features = df_daily.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_features = df_daily.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"Numeric features:\", numerical_features)\n",
    "print(\"Categorical features:\", categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "77e9ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily['datetime'] = pd.to_datetime(df_daily['datetime'])\n",
    "df_daily = df_daily.sort_values('datetime').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9642c5",
   "metadata": {},
   "source": [
    "**2. Drop missing columns**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dbd25ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng giá trị thiếu:\n",
      " datetime               0\n",
      "tempmax                0\n",
      "tempmin                0\n",
      "temp                   0\n",
      "feelslikemax           0\n",
      "feelslikemin           0\n",
      "feelslike              0\n",
      "dew                    0\n",
      "humidity               0\n",
      "precip                 0\n",
      "precipcover            0\n",
      "windgust               0\n",
      "windspeed              0\n",
      "winddir                0\n",
      "sealevelpressure       0\n",
      "cloudcover             0\n",
      "visibility             0\n",
      "solarradiation         0\n",
      "solarenergy            0\n",
      "uvindex                0\n",
      "severerisk          2566\n",
      "sunrise                0\n",
      "sunset                 0\n",
      "moonphase              0\n",
      "conditions             0\n",
      "description            0\n",
      "icon                   0\n",
      "stations               0\n",
      "dtype: int64\n",
      "\n",
      "Tỷ lệ thiếu (%):\n",
      " datetime             0.00\n",
      "tempmax              0.00\n",
      "tempmin              0.00\n",
      "temp                 0.00\n",
      "feelslikemax         0.00\n",
      "feelslikemin         0.00\n",
      "feelslike            0.00\n",
      "dew                  0.00\n",
      "humidity             0.00\n",
      "precip               0.00\n",
      "precipcover          0.00\n",
      "windgust             0.00\n",
      "windspeed            0.00\n",
      "winddir              0.00\n",
      "sealevelpressure     0.00\n",
      "cloudcover           0.00\n",
      "visibility           0.00\n",
      "solarradiation       0.00\n",
      "solarenergy          0.00\n",
      "uvindex              0.00\n",
      "severerisk          65.34\n",
      "sunrise              0.00\n",
      "sunset               0.00\n",
      "moonphase            0.00\n",
      "conditions           0.00\n",
      "description          0.00\n",
      "icon                 0.00\n",
      "stations             0.00\n",
      "dtype: float64\n",
      "\n",
      "Số dòng trùng lặp: 0\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra missing values và duplicates\n",
    "print(\"Số lượng giá trị thiếu:\\n\", df_daily.isnull().sum())\n",
    "print(\"\\nTỷ lệ thiếu (%):\\n\", (df_daily.isnull().mean() * 100).round(2))\n",
    "\n",
    "dupes_daily = df_daily.duplicated().sum()\n",
    "print(f\"\\nSố dòng trùng lặp: {dupes_daily}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ed049",
   "metadata": {},
   "source": [
    "During the data exploration process, I observed that **only the `severerisk` column contains missing values**, with a **missing rate as high as 65.34%**. Such a high proportion of missing data suggests that the column provides limited informational value and may negatively impact model performance if retained. Furthermore, correlation analysis shows that `severerisk` has little to no relationship with the target variable. Keeping it would add noise rather than value to the model.  \n",
    "\n",
    "Given the high missing rate, weak correlation, and lack of statistical reliability,  \n",
    "the `severerisk` column will be **dropped** from the dataset prior to feature engineering and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f55906de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước dữ liệu sau khi drop cột 'severerisk': (3927, 27)\n"
     ]
    }
   ],
   "source": [
    "# Loại bỏ cột severisk\n",
    "df_daily = df_daily.drop(columns=['severerisk'])\n",
    "# Kiểm tra lại kết quả\n",
    "print(\"Kích thước dữ liệu sau khi drop cột 'severerisk':\", df_daily.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f63d70",
   "metadata": {},
   "source": [
    "**3. Train-test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "75e9e270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước tập huấn luyện: (3134, 27)\n",
      "Kích thước tập kiểm tra: (786, 27)\n",
      "Khoảng thời gian tập huấn luyện: 2015-01-01 00:00:00 đến 2023-07-31 00:00:00\n",
      "Khoảng thời gian tập kiểm tra: 2023-08-08 00:00:00 đến 2025-10-01 00:00:00\n",
      "X_train shape: (3134, 25), y_train shape: (3134,)\n",
      "X_test shape: (786, 25), y_test shape: (786,)\n"
     ]
    }
   ],
   "source": [
    "# Chia train-test split theo thời gian, trong đó khoảng cách tập train và test là 7 ngày để tránh data leakage do tính chất thời gian của dữ liệu, temp là biến mục tiêu\n",
    "train_size = int(len(df_daily) * 0.8)\n",
    "# Tập huấn luyện kết thúc trước 7 ngày so với tập kiểm tra\n",
    "train_data = df_daily.iloc[:train_size - 7] \n",
    "# Tập kiểm tra bắt đầu từ ngày thứ 7 sau tập huấn luyện đến hết\n",
    "test_data = df_daily.iloc[train_size:]\n",
    "print(f\"Kích thước tập huấn luyện: {train_data.shape}\")\n",
    "print(f\"Kích thước tập kiểm tra: {test_data.shape}\")\n",
    "print(f\"Khoảng thời gian tập huấn luyện: {train_data['datetime'].min()} đến {train_data['datetime'].max()}\")\n",
    "print(f\"Khoảng thời gian tập kiểm tra: {test_data['datetime'].min()} đến {test_data['datetime'].max()}\")\n",
    "\n",
    "# temp là biến mục tiêu\n",
    "feature_cols = df_daily.columns.drop(['temp', 'datetime'])  # Loại bỏ cột temp khỏi features\n",
    "X_train = train_data[feature_cols]\n",
    "y_train = train_data['temp']\n",
    "X_test = test_data[feature_cols]\n",
    "y_test = test_data['temp']\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2041f024",
   "metadata": {},
   "source": [
    "**4. Observing and handling outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0e23be23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng outliers (theo từng cột):\n",
      " tempmax               3\n",
      "tempmin               1\n",
      "temp                  2\n",
      "feelslikemax          0\n",
      "feelslikemin         20\n",
      "feelslike             0\n",
      "dew                  41\n",
      "humidity             91\n",
      "precip              639\n",
      "precipcover         169\n",
      "windgust            117\n",
      "windspeed            59\n",
      "winddir             664\n",
      "sealevelpressure      1\n",
      "cloudcover            0\n",
      "visibility          119\n",
      "solarradiation        0\n",
      "solarenergy           0\n",
      "uvindex               0\n",
      "moonphase             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# # Phát hiện outliers bằng phương pháp IQR\n",
    "numeric_cols_d = df_daily.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "Q1_d = df_daily[numeric_cols_d].quantile(0.25)\n",
    "Q3_d = df_daily[numeric_cols_d].quantile(0.75)\n",
    "IQR_d = Q3_d - Q1_d\n",
    "\n",
    "outliers_d = ((df_daily[numeric_cols_d] < (Q1_d - 1.5 * IQR_d)) | \n",
    "              (df_daily[numeric_cols_d] > (Q3_d + 1.5 * IQR_d))).sum()\n",
    "\n",
    "print(\"Số lượng outliers (theo từng cột):\\n\", outliers_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c79032dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Phân loại cột dựa trên outliers ===\n",
      "\n",
      "Cột có nhiều outliers (>5%): ['precip', 'winddir']\n",
      "\n",
      "Cột có ít outliers (<=5%): ['tempmax', 'tempmin', 'feelslikemax', 'feelslikemin', 'feelslike', 'dew', 'humidity', 'precipcover', 'windgust', 'windspeed', 'sealevelpressure', 'cloudcover', 'visibility', 'solarradiation', 'solarenergy', 'uvindex', 'moonphase']\n",
      "\n",
      "Tỷ lệ outliers chi tiết:\n",
      "winddir             0.166879\n",
      "precip              0.166879\n",
      "precipcover         0.037652\n",
      "windgust            0.029036\n",
      "visibility          0.027760\n",
      "humidity            0.020740\n",
      "windspeed           0.012763\n",
      "cloudcover          0.010530\n",
      "dew                 0.009572\n",
      "feelslikemin        0.006063\n",
      "sealevelpressure    0.000638\n",
      "tempmax             0.000638\n",
      "tempmin             0.000319\n",
      "feelslike           0.000000\n",
      "feelslikemax        0.000000\n",
      "solarradiation      0.000000\n",
      "solarenergy         0.000000\n",
      "uvindex             0.000000\n",
      "moonphase           0.000000\n",
      "dtype: float64\n",
      "\n",
      "Cột nominal: ['conditions', 'description', 'icon', 'stations']\n",
      "Cột ordinal: []\n"
     ]
    }
   ],
   "source": [
    "# Phân loại các cột dựa trên số lượng outliers\n",
    "def classify_columns_by_outliers(X, outlier_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Phân loại cột numeric thành high outliers và low outliers\n",
    "    outlier_threshold: tỷ lệ outliers để phân loại (mặc định 5%)\n",
    "    \"\"\"\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    Q1 = X[numeric_cols].quantile(0.25)\n",
    "    Q3 = X[numeric_cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    outlier_counts = ((X[numeric_cols] < (Q1 - 1.5 * IQR)) | \n",
    "                      (X[numeric_cols] > (Q3 + 1.5 * IQR))).sum()\n",
    "    outlier_ratios = outlier_counts / len(X)\n",
    "    \n",
    "    high_outlier_cols = outlier_ratios[outlier_ratios > outlier_threshold].index.tolist()\n",
    "    low_outlier_cols = outlier_ratios[outlier_ratios <= outlier_threshold].index.tolist()\n",
    "    \n",
    "    return high_outlier_cols, low_outlier_cols, outlier_ratios\n",
    "\n",
    "# Phân loại cột cho dữ liệu daily\n",
    "high_outlier_cols, low_outlier_cols, outlier_ratios = classify_columns_by_outliers(X_train, outlier_threshold=0.05)\n",
    "\n",
    "print(\"=== Phân loại cột dựa trên outliers ===\")\n",
    "print(f\"\\nCột có nhiều outliers (>5%): {high_outlier_cols}\")\n",
    "print(f\"\\nCột có ít outliers (<=5%): {low_outlier_cols}\")\n",
    "print(f\"\\nTỷ lệ outliers chi tiết:\\n{outlier_ratios.sort_values(ascending=False)}\")\n",
    "\n",
    "# Xác định cột categorical\n",
    "nominal_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "ordinal_cols = []  # Nếu có cột ordinal, khai báo ở đây\n",
    "\n",
    "print(f\"\\nCột nominal: {nominal_cols}\")\n",
    "print(f\"Cột ordinal: {ordinal_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f9573db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bắt đầu fit và transform dữ liệu ===\n",
      "\n",
      "Kích thước sau khi transform:\n",
      "  X_train: (3134, 25) -> (3134, 78)\n",
      "  X_test: (786, 25) -> (786, 78)\n",
      "\n",
      "Kích thước sau khi thêm cột datetime vào:\n",
      "  X_train: (3134, 80)\n",
      "  X_test: (786, 80)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler, StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# Tách các cột datetime\n",
    "datetime_cols = X_train.select_dtypes(include=['datetime']).columns.tolist()\n",
    "\n",
    "# Tách các cột không phải datetime (numeric và categorical)\n",
    "train_non_datetime = train_data.drop(columns=datetime_cols)\n",
    "test_non_datetime = test_data.drop(columns=datetime_cols)\n",
    "\n",
    "# Tạo ColumnTransformer\n",
    "transformers = []\n",
    "\n",
    "# 1. RobustScaler cho numeric có nhiều outliers\n",
    "if high_outlier_cols:\n",
    "    transformers.append(('robust', RobustScaler(), high_outlier_cols))\n",
    "\n",
    "# 2. StandardScaler cho numeric có ít outliers\n",
    "if low_outlier_cols:\n",
    "    transformers.append(('standard', StandardScaler(), low_outlier_cols))\n",
    "\n",
    "# 3. OneHotEncoder cho nominal\n",
    "if nominal_cols:\n",
    "    transformers.append(('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), nominal_cols))\n",
    "\n",
    "# 4. OrdinalEncoder cho ordinal (nếu có)\n",
    "if ordinal_cols:\n",
    "    transformers.append(('ordinal', OrdinalEncoder(), ordinal_cols))\n",
    "\n",
    "# Tạo ColumnTransformer (passthrough để giữ nguyên các cột không được transform)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    remainder='passthrough', verbose_feature_names_out=False  # Giữ nguyên các cột không được transform\n",
    ")\n",
    "\n",
    "# ===== FIT VÀ TRANSFORM =====\n",
    "print(\"\\n=== Bắt đầu fit và transform dữ liệu ===\")\n",
    "train_transformed = preprocessor.fit_transform(train_non_datetime)\n",
    "\n",
    "# Transform tập TEST (dùng parameters đã học từ train)\n",
    "test_transformed = preprocessor.transform(test_non_datetime)\n",
    "\n",
    "print(f\"\\nKích thước sau khi transform:\")\n",
    "print(f\"  X_train: {train_non_datetime.shape} -> {train_transformed.shape}\")\n",
    "print(f\"  X_test: {test_non_datetime.shape} -> {test_transformed.shape}\")\n",
    "\n",
    "# Kết hợp lại các cột datetime vào kết quả cuối cùng\n",
    "# Đảm bảo rằng các cột datetime sẽ không bị thay đổi\n",
    "train_transformed = pd.DataFrame(train_transformed)\n",
    "test_transformed = pd.DataFrame(test_transformed)\n",
    "\n",
    "# Thêm lại cột datetime vào kết quả cuối cùng\n",
    "train_transformed[datetime_cols] = train_data[datetime_cols].reset_index(drop=True)\n",
    "test_transformed[datetime_cols] = test_data[datetime_cols].reset_index(drop=True)\n",
    "\n",
    "# Hiển thị kết quả\n",
    "print(f\"\\nKích thước sau khi thêm cột datetime vào:\")\n",
    "print(f\"  X_train: {train_transformed.shape}\")\n",
    "print(f\"  X_test: {test_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "afaca042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "# ===== Sửa lỗi: fit/transform + giữ tên cột + giữ datetime =====\n",
    "import os\n",
    "from pandas.api import types as pd_types\n",
    "\n",
    "# Fit trên dữ liệu không chứa datetime (nếu chưa fit)\n",
    "preprocessor.fit(train_non_datetime)\n",
    "\n",
    "train_arr = preprocessor.transform(train_non_datetime)\n",
    "test_arr = preprocessor.transform(test_non_datetime)\n",
    "\n",
    "# Tạo tên feature an toàn\n",
    "try:\n",
    "    # sklearn >=1.0\n",
    "    feature_names = list(preprocessor.get_feature_names_out(train_non_datetime.columns))\n",
    "except Exception:\n",
    "    feature_names = []\n",
    "    used_cols = []\n",
    "    for name, trans, cols in preprocessor.transformers_:\n",
    "        if name == 'remainder':\n",
    "            continue\n",
    "        cols_list = list(cols) if isinstance(cols, (list, tuple, np.ndarray)) else [cols]\n",
    "        used_cols.extend(cols_list)\n",
    "        fitted = preprocessor.named_transformers_.get(name, None)\n",
    "        if fitted is not None and hasattr(fitted, \"get_feature_names_out\"):\n",
    "            try:\n",
    "                out = fitted.get_feature_names_out(cols_list)\n",
    "                feature_names.extend(list(out))\n",
    "            except Exception:\n",
    "                feature_names.extend([f\"{name}__{c}\" for c in cols_list])\n",
    "        else:\n",
    "            feature_names.extend([f\"{name}__{c}\" for c in cols_list])\n",
    "    # thêm passthrough nếu có\n",
    "    if getattr(preprocessor, \"remainder\", None) == 'passthrough':\n",
    "        passthrough = [c for c in train_non_datetime.columns if c not in used_cols]\n",
    "        feature_names.extend(passthrough)\n",
    "\n",
    "# Fallback nếu mismatch kích thước\n",
    "if len(feature_names) != train_arr.shape[1]:\n",
    "    feature_names = [f\"feat_{i}\" for i in range(train_arr.shape[1])]\n",
    "\n",
    "# Chuyển về DataFrame và giữ index gốc\n",
    "train_transformed = pd.DataFrame(train_arr, columns=feature_names, index=train_non_datetime.index)\n",
    "test_transformed = pd.DataFrame(test_arr, columns=feature_names, index=test_non_datetime.index)\n",
    "\n",
    "# Lấy cột datetime từ train_data/test_data (giữ dtype)\n",
    "datetime_cols = [c for c in train_data.columns if pd_types.is_datetime64_any_dtype(train_data[c])]\n",
    "\n",
    "for dt in datetime_cols:\n",
    "    # nếu tên datetime vô tình có trong feature_names thì ghi đè bằng giá trị gốc\n",
    "    train_transformed[dt] = train_data.loc[train_transformed.index, dt].values\n",
    "    test_transformed[dt] = test_data.loc[test_transformed.index, dt].values\n",
    "\n",
    "# Đảm bảo thứ tự cột: feature_names (không chứa datetime) + datetime_cols\n",
    "final_feature_names = [c for c in feature_names if c not in datetime_cols] + datetime_cols\n",
    "train_transformed = train_transformed[final_feature_names]\n",
    "test_transformed = test_transformed[final_feature_names]\n",
    "\n",
    "# Thêm target 'temp' lại vào nếu cần và lưu\n",
    "train_data_transformed = train_transformed.copy()\n",
    "train_data_transformed['temp'] = train_data.loc[train_transformed.index, 'temp'].values\n",
    "test_data_transformed = test_transformed.copy()\n",
    "test_data_transformed['temp'] = test_data.loc[test_transformed.index, 'temp'].values\n",
    "\n",
    "save_dir = '../data/processed/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "train_data_transformed.to_excel(save_dir + 'train_data.xlsx', index=False)\n",
    "test_data_transformed.to_excel(save_dir + 'test_data.xlsx', index=False)\n",
    "X_train_transformed = train_transformed.copy()\n",
    "X_test_transformed = test_transformed.copy()\n",
    "X_train_transformed.to_excel(save_dir + 'X_train.xlsx', index=False)\n",
    "X_test_transformed.to_excel(save_dir + 'X_test.xlsx', index=False)\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "28612b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data (train_data_transformed / test_data_transformed already created above)\n",
    "import os\n",
    "save_dir = '../data/processed/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save raw copies if needed\n",
    "train_data.to_excel(save_dir + 'train_data_raw.xlsx', index=False)\n",
    "test_data.to_excel(save_dir + 'test_data_raw.xlsx', index=False)\n",
    "\n",
    "# Helper to convert transformed array -> DataFrame with sensible column names\n",
    "def to_df(arr, index, fallback_prefix='feature'):\n",
    "    if isinstance(arr, np.ndarray):\n",
    "        if 'feature_names_out' in globals() and len(feature_names_out) == arr.shape[1]:\n",
    "            cols = feature_names_out\n",
    "        else:\n",
    "            cols = [f'{fallback_prefix}_{i}' for i in range(arr.shape[1])]\n",
    "        return pd.DataFrame(arr, columns=cols, index=index)\n",
    "    elif isinstance(arr, pd.DataFrame):\n",
    "        return arr.copy()\n",
    "    else:\n",
    "        raise TypeError(\"Transformed data must be numpy.ndarray or pandas.DataFrame\")\n",
    "\n",
    "# Convert and save train_data_transformed / test_data_transformed (no X/y combine)\n",
    "train_data.to_excel(save_dir + 'train_data.xlsx', index=False)\n",
    "test_data.to_excel(save_dir + 'test_data.xlsx', index=False)\n",
    "\n",
    "train_index = X_train.index if isinstance(X_train, pd.DataFrame) else range(X_train.shape[0])\n",
    "test_index  = X_test.index  if isinstance(X_test, pd.DataFrame)  else range(X_test.shape[0])\n",
    "\n",
    "# Chuyển X_train_transformed / X_test_transformed -> DataFrame\n",
    "if 'X_train_transformed' in globals():\n",
    "    train_data_transformed_df = to_df(X_train_transformed, index=train_index, fallback_prefix='Xtrain_feat')\n",
    "    train_data_transformed_df.to_excel(save_dir + 'train_data_transformed.xlsx', index=False)\n",
    "\n",
    "if 'X_test_transformed' in globals():\n",
    "    test_data_transformed_df = to_df(X_test_transformed, index=test_index, fallback_prefix='Xtest_feat')\n",
    "    test_data_transformed_df.to_excel(save_dir + 'test_data_transformed.xlsx', index=False)\n",
    "\n",
    "train_data_transformed_df.to_excel(save_dir + 'train_data.xlsx', index=False)\n",
    "test_data_transformed_df.to_excel(save_dir + 'test_data.xlsx', index=False)\n",
    "\n",
    "# Chuyển y_train / y_test thành Series nếu đang là ndarray\n",
    "y_train_series = pd.Series(y_train) if isinstance(y_train, np.ndarray) else y_train.squeeze()\n",
    "y_test_series  = pd.Series(y_test)  if isinstance(y_test, np.ndarray)  else y_test.squeeze()\n",
    "\n",
    "# Đặt tên cột nếu chưa có\n",
    "if y_train_series.name is None:\n",
    "    y_train_series.name = 'target'\n",
    "if y_test_series.name is None:\n",
    "    y_test_series.name = 'target'\n",
    "\n",
    "# Lưu ra Excel\n",
    "y_train_series.to_frame().to_excel(save_dir + 'y_train.xlsx', index=False)\n",
    "y_test_series.to_frame().to_excel(save_dir + 'y_test.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ac9bd80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = train_transformed.columns.drop(['temp', 'datetime']) \n",
    "X_train_transformed = train_transformed[feature_cols]\n",
    "y_train_transformed = train_transformed['temp']\n",
    "X_test_transformed = test_transformed[feature_cols]\n",
    "y_test_transformed = test_transformed['temp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f182f",
   "metadata": {},
   "source": [
    "**TRAIN MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7cc96b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (3123, 7)\n",
      "y shape: (3123, 5)\n",
      "Ngày 1: MSE=2.9767, MAE=1.2704\n",
      "Ngày 2: MSE=6.2263, MAE=1.9013\n",
      "Ngày 3: MSE=7.5937, MAE=2.1234\n",
      "Ngày 4: MSE=8.4063, MAE=2.2642\n",
      "Ngày 5: MSE=9.1083, MAE=2.3556\n",
      "Trung bình MSE: 6.8623\n",
      "Trung bình MAE: 1.9830\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "target = train_data_transformed['temp'].values\n",
    "\n",
    "n_past = 7\n",
    "n_future = 5\n",
    "\n",
    "X, y = [], []\n",
    "for i in range(len(target) - n_past - n_future + 1):\n",
    "    X.append(target[i:i+n_past])\n",
    "    y.append(target[i+n_past:i+n_past+n_future])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train_seq, X_test_seq = X[:split_idx], X[split_idx:]\n",
    "y_train_seq, y_test_seq = y[:split_idx], y[split_idx:]\n",
    "\n",
    "base_model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "multi_model = MultiOutputRegressor(base_model)\n",
    "\n",
    "multi_model.fit(X_train_seq, y_train_seq)\n",
    "\n",
    "y_pred_seq = multi_model.predict(X_test_seq)\n",
    "\n",
    "mse = mean_squared_error(y_test_seq, y_pred_seq, multioutput='raw_values')\n",
    "mae = mean_absolute_error(y_test_seq, y_pred_seq, multioutput='raw_values')\n",
    "\n",
    "for i in range(n_future):\n",
    "    print(f\"Ngày {i+1}: MSE={mse[i]:.4f}, MAE={mae[i]:.4f}\")\n",
    "\n",
    "print(f\"Trung bình MSE: {np.mean(mse):.4f}\")\n",
    "print(f\"Trung bình MAE: {np.mean(mae):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
